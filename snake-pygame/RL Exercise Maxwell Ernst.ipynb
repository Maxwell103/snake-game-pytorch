{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL Exercise "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The agent uses a neural network, defined in the model module as Linear_QNet, to approximate the Q-values for different game states. The agent interacts with the game environment, represented by the SnakeGameAI class, and uses the QTrainer class to train the neural network."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key components of the agent are as follows:\n",
    "\n",
    "Agent class: Represents the agent that plays the Snake game. It has methods to get the current game state, choose an action, remember the experience, and train the neural network using long and short-term memory.\n",
    "\n",
    "get_state() method: Gets the current game state by observing the game environment. It collects information such as the direction of the snake, presence of danger (collision with walls or body), and location of food, and returns it as a numpy array.\n",
    "\n",
    "remember() method: Stores the current experience (state, action, reward, next state, and done flag) in the agent's memory. The memory has a maximum capacity defined by MAX_MEMORY.\n",
    "\n",
    "train_long_memory() and train_short_memory() methods: Train the neural network using experiences stored in the agent's memory. train_long_memory() is called after each game is completed and trains the network with a batch of experiences from the memory. train_short_memory() is called after each step in the game and trains the network with a single experience.\n",
    "\n",
    "get_action() method: Chooses an action for the agent based on the current game state. The agent has an epsilon-greedy exploration-exploitation strategy, where it chooses a random action with probability epsilon and chooses the action with the highest Q-value predicted by the neural network with probability 1-epsilon.\n",
    "\n",
    "train() function: The main training loop that runs the game episodes. It interacts with the game environment, gets the current state, chooses an action, performs the action, and updates the neural network and memory accordingly. It also plots the game scores and mean scores over time.\n",
    "\n",
    "Linear_QNet class: Represents the neural network architecture used by the agent. It is a simple feedforward neural network with linear layers, and takes the current game state as input and outputs Q-values for different actions. It has methods to save and load the model weights.\n",
    "\n",
    "QTrainer class: Represents the trainer for the neural network. It uses the Adam optimizer to update the weights based on the Q-learning loss function. It has a method train_step() that takes the current state, action, reward, next state, and done flag as inputs, calculates the loss, and updates the weights of the neural network."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial run"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](images/Screenshot%202023-04-15%20005406.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixed an error in  the model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](images/Screenshot%202023-04-15%20003146.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](images/Screenshot%202023-04-15%20001305.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Highest score for over 250 games"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](images/Screenshot%202023-04-15%20145437.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](images/plot_firsttune.gif)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the average score for the first about 100 games is much higher than before. This shows that the model is performing better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
